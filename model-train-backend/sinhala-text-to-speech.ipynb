{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import librosa\n",
    "import numpy as np\n",
    "from gtts import gTTS\n",
    "import torch, json, os\n",
    "import soundfile as sf\n",
    "from tokenizers import Tokenizer\n",
    "from dataclasses import dataclass\n",
    "from collections import defaultdict\n",
    "from datasets import load_dataset, Audio\n",
    "from typing import Any, Dict, List, Union\n",
    "from datasets import Audio, Dataset, Value, Features, load_dataset\n",
    "from transformers import SpeechT5Processor, SpeechT5ForTextToSpeech, SpeechT5HifiGan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9497896b1eb4ac9bd96d3096c9cad82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/585M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thiro\\anaconda3\\envs\\myenv\\lib\\site-packages\\huggingface_hub\\file_download.py:137: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\thiro\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "processor = SpeechT5Processor.from_pretrained(\"microsoft/speecht5_tts\")\n",
    "model = SpeechT5ForTextToSpeech.from_pretrained(\"microsoft/speecht5_tts\")\n",
    "vocoder = SpeechT5HifiGan.from_pretrained(\"microsoft/speecht5_hifigan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SinhalaTTSdataset(mapping_json = 'data/tts/file-mapping.json'):\n",
    "    with open(mapping_json) as f:\n",
    "        mapping = json.load(f)\n",
    "    \n",
    "    data = {}\n",
    "    data[\"audio\"] = []\n",
    "    data[\"normalized_text\"] = []\n",
    "    for val_dict in mapping.values():\n",
    "        audio_path = f\"data/tts/wavs/{val_dict['newfn']}\"\n",
    "        sinhala_text = val_dict['sinhala']\n",
    "        if os.path.exists(audio_path):\n",
    "            audio, sampling_rate = librosa.load(audio_path, sr=16000)\n",
    "            audio_data = {\n",
    "                            \"path\": audio_path,\n",
    "                            \"array\": audio,\n",
    "                            \"sampling_rate\": sampling_rate\n",
    "                         }\n",
    "            data[\"audio\"].append(audio_data)\n",
    "            data[\"normalized_text\"].append(sinhala_text)\n",
    "\n",
    "    return Dataset.from_dict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    dataset = SinhalaTTSdataset()\n",
    "except:\n",
    "    dataset = load_dataset(\n",
    "                        \"facebook/voxpopuli\", \n",
    "                        \"nl\", split=\"train\"\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\thiro\\AudioSight\\model-train-backend\\sinhala-text-to-speech.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/thiro/AudioSight/model-train-backend/sinhala-text-to-speech.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m dataset \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39;49mcast_column(\u001b[39m\"\u001b[39;49m\u001b[39maudio\u001b[39;49m\u001b[39m\"\u001b[39;49m, Audio(sampling_rate\u001b[39m=\u001b[39;49m\u001b[39m16000\u001b[39;49m))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/thiro/AudioSight/model-train-backend/sinhala-text-to-speech.ipynb#W5sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m processor\u001b[39m.\u001b[39mtokenizer\n",
      "File \u001b[1;32mc:\\Users\\thiro\\anaconda3\\envs\\myenv\\lib\\site-packages\\datasets\\fingerprint.py:511\u001b[0m, in \u001b[0;36mfingerprint_transform.<locals>._fingerprint.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    507\u001b[0m             validate_fingerprint(kwargs[fingerprint_name])\n\u001b[0;32m    509\u001b[0m \u001b[39m# Call actual function\u001b[39;00m\n\u001b[1;32m--> 511\u001b[0m out \u001b[39m=\u001b[39m func(dataset, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    513\u001b[0m \u001b[39m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[39;00m\n\u001b[0;32m    515\u001b[0m \u001b[39mif\u001b[39;00m inplace:  \u001b[39m# update after calling func so that the fingerprint doesn't change if the function fails\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\thiro\\anaconda3\\envs\\myenv\\lib\\site-packages\\datasets\\arrow_dataset.py:2104\u001b[0m, in \u001b[0;36mDataset.cast_column\u001b[1;34m(self, column, feature, new_fingerprint)\u001b[0m\n\u001b[0;32m   2102\u001b[0m dataset\u001b[39m.\u001b[39m_info\u001b[39m.\u001b[39mfeatures[column] \u001b[39m=\u001b[39m feature\n\u001b[0;32m   2103\u001b[0m dataset\u001b[39m.\u001b[39m_fingerprint \u001b[39m=\u001b[39m new_fingerprint\n\u001b[1;32m-> 2104\u001b[0m dataset\u001b[39m.\u001b[39m_data \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39;49m_data\u001b[39m.\u001b[39;49mcast(dataset\u001b[39m.\u001b[39;49mfeatures\u001b[39m.\u001b[39;49marrow_schema)\n\u001b[0;32m   2105\u001b[0m dataset\u001b[39m.\u001b[39m_data \u001b[39m=\u001b[39m update_metadata_with_features(dataset\u001b[39m.\u001b[39m_data, dataset\u001b[39m.\u001b[39mfeatures)\n\u001b[0;32m   2106\u001b[0m \u001b[39mreturn\u001b[39;00m dataset\n",
      "File \u001b[1;32mc:\\Users\\thiro\\anaconda3\\envs\\myenv\\lib\\site-packages\\datasets\\table.py:901\u001b[0m, in \u001b[0;36mInMemoryTable.cast\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    888\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcast\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    889\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    890\u001b[0m \u001b[39m    Cast table values to another schema.\u001b[39;00m\n\u001b[0;32m    891\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    899\u001b[0m \u001b[39m        `datasets.table.Table`\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 901\u001b[0m     \u001b[39mreturn\u001b[39;00m InMemoryTable(table_cast(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtable, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs))\n",
      "File \u001b[1;32mc:\\Users\\thiro\\anaconda3\\envs\\myenv\\lib\\site-packages\\datasets\\table.py:2328\u001b[0m, in \u001b[0;36mtable_cast\u001b[1;34m(table, schema)\u001b[0m\n\u001b[0;32m   2314\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Improved version of `pa.Table.cast`.\u001b[39;00m\n\u001b[0;32m   2315\u001b[0m \n\u001b[0;32m   2316\u001b[0m \u001b[39mIt supports casting to feature types stored in the schema metadata.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2325\u001b[0m \u001b[39m    table (`pyarrow.Table`): the casted table\u001b[39;00m\n\u001b[0;32m   2326\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   2327\u001b[0m \u001b[39mif\u001b[39;00m table\u001b[39m.\u001b[39mschema \u001b[39m!=\u001b[39m schema:\n\u001b[1;32m-> 2328\u001b[0m     \u001b[39mreturn\u001b[39;00m cast_table_to_schema(table, schema)\n\u001b[0;32m   2329\u001b[0m \u001b[39melif\u001b[39;00m table\u001b[39m.\u001b[39mschema\u001b[39m.\u001b[39mmetadata \u001b[39m!=\u001b[39m schema\u001b[39m.\u001b[39mmetadata:\n\u001b[0;32m   2330\u001b[0m     \u001b[39mreturn\u001b[39;00m table\u001b[39m.\u001b[39mreplace_schema_metadata(schema\u001b[39m.\u001b[39mmetadata)\n",
      "File \u001b[1;32mc:\\Users\\thiro\\anaconda3\\envs\\myenv\\lib\\site-packages\\datasets\\table.py:2287\u001b[0m, in \u001b[0;36mcast_table_to_schema\u001b[1;34m(table, schema)\u001b[0m\n\u001b[0;32m   2285\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39msorted\u001b[39m(table\u001b[39m.\u001b[39mcolumn_names) \u001b[39m!=\u001b[39m \u001b[39msorted\u001b[39m(features):\n\u001b[0;32m   2286\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCouldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt cast\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mtable\u001b[39m.\u001b[39mschema\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mto\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mfeatures\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mbecause column names don\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt match\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 2287\u001b[0m arrays \u001b[39m=\u001b[39m [cast_array_to_feature(table[name], feature) \u001b[39mfor\u001b[39;00m name, feature \u001b[39min\u001b[39;00m features\u001b[39m.\u001b[39mitems()]\n\u001b[0;32m   2288\u001b[0m \u001b[39mreturn\u001b[39;00m pa\u001b[39m.\u001b[39mTable\u001b[39m.\u001b[39mfrom_arrays(arrays, schema\u001b[39m=\u001b[39mschema)\n",
      "File \u001b[1;32mc:\\Users\\thiro\\anaconda3\\envs\\myenv\\lib\\site-packages\\datasets\\table.py:2287\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   2285\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39msorted\u001b[39m(table\u001b[39m.\u001b[39mcolumn_names) \u001b[39m!=\u001b[39m \u001b[39msorted\u001b[39m(features):\n\u001b[0;32m   2286\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCouldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt cast\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mtable\u001b[39m.\u001b[39mschema\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mto\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mfeatures\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mbecause column names don\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt match\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 2287\u001b[0m arrays \u001b[39m=\u001b[39m [cast_array_to_feature(table[name], feature) \u001b[39mfor\u001b[39;00m name, feature \u001b[39min\u001b[39;00m features\u001b[39m.\u001b[39mitems()]\n\u001b[0;32m   2288\u001b[0m \u001b[39mreturn\u001b[39;00m pa\u001b[39m.\u001b[39mTable\u001b[39m.\u001b[39mfrom_arrays(arrays, schema\u001b[39m=\u001b[39mschema)\n",
      "File \u001b[1;32mc:\\Users\\thiro\\anaconda3\\envs\\myenv\\lib\\site-packages\\datasets\\table.py:1831\u001b[0m, in \u001b[0;36m_wrap_for_chunked_arrays.<locals>.wrapper\u001b[1;34m(array, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1829\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(array, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m   1830\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(array, pa\u001b[39m.\u001b[39mChunkedArray):\n\u001b[1;32m-> 1831\u001b[0m         \u001b[39mreturn\u001b[39;00m pa\u001b[39m.\u001b[39mchunked_array([func(chunk, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m array\u001b[39m.\u001b[39mchunks])\n\u001b[0;32m   1832\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1833\u001b[0m         \u001b[39mreturn\u001b[39;00m func(array, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\thiro\\anaconda3\\envs\\myenv\\lib\\site-packages\\datasets\\table.py:1831\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1829\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(array, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m   1830\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(array, pa\u001b[39m.\u001b[39mChunkedArray):\n\u001b[1;32m-> 1831\u001b[0m         \u001b[39mreturn\u001b[39;00m pa\u001b[39m.\u001b[39mchunked_array([func(chunk, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs) \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m array\u001b[39m.\u001b[39mchunks])\n\u001b[0;32m   1832\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1833\u001b[0m         \u001b[39mreturn\u001b[39;00m func(array, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\thiro\\anaconda3\\envs\\myenv\\lib\\site-packages\\datasets\\table.py:2063\u001b[0m, in \u001b[0;36mcast_array_to_feature\u001b[1;34m(array, feature, allow_number_to_str)\u001b[0m\n\u001b[0;32m   2061\u001b[0m     array \u001b[39m=\u001b[39m array\u001b[39m.\u001b[39mstorage\n\u001b[0;32m   2062\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(feature, \u001b[39m\"\u001b[39m\u001b[39mcast_storage\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m-> 2063\u001b[0m     \u001b[39mreturn\u001b[39;00m feature\u001b[39m.\u001b[39;49mcast_storage(array)\n\u001b[0;32m   2064\u001b[0m \u001b[39melif\u001b[39;00m pa\u001b[39m.\u001b[39mtypes\u001b[39m.\u001b[39mis_struct(array\u001b[39m.\u001b[39mtype):\n\u001b[0;32m   2065\u001b[0m     \u001b[39m# feature must be a dict or Sequence(subfeatures_dict)\u001b[39;00m\n\u001b[0;32m   2066\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(feature, Sequence) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(feature\u001b[39m.\u001b[39mfeature, \u001b[39mdict\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\thiro\\anaconda3\\envs\\myenv\\lib\\site-packages\\datasets\\features\\audio.py:237\u001b[0m, in \u001b[0;36mAudio.cast_storage\u001b[1;34m(self, storage)\u001b[0m\n\u001b[0;32m    235\u001b[0m     storage \u001b[39m=\u001b[39m pa\u001b[39m.\u001b[39mStructArray\u001b[39m.\u001b[39mfrom_arrays([storage, path_array], [\u001b[39m\"\u001b[39m\u001b[39mbytes\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mpath\u001b[39m\u001b[39m\"\u001b[39m], mask\u001b[39m=\u001b[39mstorage\u001b[39m.\u001b[39mis_null())\n\u001b[0;32m    236\u001b[0m \u001b[39melif\u001b[39;00m pa\u001b[39m.\u001b[39mtypes\u001b[39m.\u001b[39mis_struct(storage\u001b[39m.\u001b[39mtype) \u001b[39mand\u001b[39;00m storage\u001b[39m.\u001b[39mtype\u001b[39m.\u001b[39mget_all_field_indices(\u001b[39m\"\u001b[39m\u001b[39marray\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 237\u001b[0m     storage \u001b[39m=\u001b[39m pa\u001b[39m.\u001b[39marray([Audio()\u001b[39m.\u001b[39mencode_example(x) \u001b[39mif\u001b[39;00m x \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m storage\u001b[39m.\u001b[39;49mto_pylist()])\n\u001b[0;32m    238\u001b[0m \u001b[39melif\u001b[39;00m pa\u001b[39m.\u001b[39mtypes\u001b[39m.\u001b[39mis_struct(storage\u001b[39m.\u001b[39mtype):\n\u001b[0;32m    239\u001b[0m     \u001b[39mif\u001b[39;00m storage\u001b[39m.\u001b[39mtype\u001b[39m.\u001b[39mget_field_index(\u001b[39m\"\u001b[39m\u001b[39mbytes\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\thiro\\anaconda3\\envs\\myenv\\lib\\site-packages\\pyarrow\\array.pxi:1567\u001b[0m, in \u001b[0;36mpyarrow.lib.Array.to_pylist\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\thiro\\anaconda3\\envs\\myenv\\lib\\site-packages\\pyarrow\\scalar.pxi:750\u001b[0m, in \u001b[0;36mpyarrow.lib.StructScalar.as_py\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\thiro\\anaconda3\\envs\\myenv\\lib\\site-packages\\pyarrow\\scalar.pxi:677\u001b[0m, in \u001b[0;36mpyarrow.lib.ListScalar.as_py\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\thiro\\anaconda3\\envs\\myenv\\lib\\site-packages\\pyarrow\\array.pxi:1567\u001b[0m, in \u001b[0;36mpyarrow.lib.Array.to_pylist\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "tokenizer = processor.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb348edd819946f398c9c4ca514d8863",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1f75c4422c145d58bfc6b5bb3f3fe4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\thiro\\AudioSight\\model-train-backend\\sinhala-text-to-speech.ipynb Cell 7\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/thiro/AudioSight/model-train-backend/sinhala-text-to-speech.ipynb#W6sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m dataset \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39mmap(cleanup_text)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/thiro/AudioSight/model-train-backend/sinhala-text-to-speech.ipynb#W6sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m dataset_vocab \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(vocabs[\u001b[39m\"\u001b[39m\u001b[39mvocab\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m])\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/thiro/AudioSight/model-train-backend/sinhala-text-to-speech.ipynb#W6sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m tokenizer_vocab \u001b[39m=\u001b[39m {k \u001b[39mfor\u001b[39;00m k,_ \u001b[39min\u001b[39;00m tokenizer\u001b[39m.\u001b[39mget_vocab()\u001b[39m.\u001b[39mitems()}\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "def extract_all_chars(batch):\n",
    "    all_text = \" \".join(batch[\"normalized_text\"])\n",
    "    vocab = list(set(all_text))\n",
    "    return {\"vocab\": [vocab], \"all_text\": [all_text]}\n",
    "\n",
    "vocabs = dataset.map(\n",
    "    extract_all_chars, \n",
    "    batched=True, \n",
    "    batch_size=-1, \n",
    "    keep_in_memory=True, \n",
    "    remove_columns=dataset.column_names,\n",
    ")\n",
    "\n",
    "replacements = [\n",
    "                ('à', 'a'),\n",
    "                ('ç', 'c'),\n",
    "                ('è', 'e'),\n",
    "                ('ë', 'e'),\n",
    "                ('í', 'i'),\n",
    "                ('ï', 'i'),\n",
    "                ('ö', 'o'),\n",
    "                ('ü', 'u'),\n",
    "                ]\n",
    "\n",
    "def cleanup_text(inputs):\n",
    "    for src, dst in replacements:\n",
    "        inputs[\"normalized_text\"] = inputs[\"normalized_text\"].replace(src, dst)\n",
    "    return inputs\n",
    "\n",
    "dataset = dataset.map(cleanup_text)\n",
    "\n",
    "dataset_vocab = set(vocabs[\"vocab\"][0])\n",
    "tokenizer_vocab = {k for k,_ in tokenizer.get_vocab().items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_speaker_embedding(waveform):\n",
    "    with torch.no_grad():\n",
    "                try:\n",
    "                    speaker_embeddings = speaker_model.encode_batch(torch.tensor(waveform))\n",
    "                    speaker_embeddings = torch.nn.functional.normalize(speaker_embeddings, dim=2)\n",
    "                    speaker_embeddings = speaker_embeddings.squeeze().cpu().numpy()\n",
    "                except:\n",
    "                    speaker_embeddings = np.random.rand(512, )\n",
    "    return speaker_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(example):\n",
    "    # load the audio data; if necessary, this resamples the audio to 16kHz\n",
    "    audio = example[\"audio\"]\n",
    "\n",
    "    # feature extraction and tokenization\n",
    "    example = processor(\n",
    "        text=example[\"normalized_text\"],\n",
    "        audio_target=audio[\"array\"], \n",
    "        sampling_rate=audio[\"sampling_rate\"],\n",
    "        return_attention_mask=False,\n",
    "    )\n",
    "\n",
    "    # strip off the batch dimension\n",
    "    example[\"labels\"] = example[\"labels\"][0]\n",
    "\n",
    "    # use SpeechBrain to obtain x-vector\n",
    "    example[\"speaker_embeddings\"] = create_speaker_embedding(audio[\"array\"])\n",
    "\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "547b4dc948b6455cbc9bc8cf8b0bc6c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe83934fa0b74af095ea8d9bdf2c145a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/3300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = dataset.map(\n",
    "    prepare_dataset, remove_columns=dataset.column_names,\n",
    ")\n",
    "\n",
    "def is_not_too_long(input_ids):\n",
    "    input_length = len(input_ids)\n",
    "    return input_length < 200\n",
    "\n",
    "dataset = dataset.filter(is_not_too_long, input_columns=[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TTSDataCollatorWithPadding:\n",
    "    processor: Any\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "\n",
    "        input_ids = [{\"input_ids\": feature[\"input_ids\"]} for feature in features]\n",
    "        label_features = [{\"input_values\": feature[\"labels\"]} for feature in features]\n",
    "        speaker_features = [feature[\"speaker_embeddings\"] for feature in features]\n",
    "\n",
    "        # collate the inputs and targets into a batch\n",
    "        batch = processor.pad(\n",
    "            input_ids=input_ids,\n",
    "            labels=label_features,\n",
    "            return_tensors=\"pt\",\n",
    "        )        \n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        batch[\"labels\"] = batch[\"labels\"].masked_fill(\n",
    "            batch.decoder_attention_mask.unsqueeze(-1).ne(1), -100\n",
    "        )\n",
    "\n",
    "        # not used during fine-tuning\n",
    "        del batch[\"decoder_attention_mask\"]\n",
    "\n",
    "        # round down target lengths to multiple of reduction factor\n",
    "        if model.config.reduction_factor > 1:\n",
    "            target_lengths = torch.tensor([\n",
    "                len(feature[\"input_values\"]) for feature in label_features\n",
    "            ])\n",
    "            target_lengths = target_lengths.new([\n",
    "                length - length % model.config.reduction_factor for length in target_lengths\n",
    "            ])\n",
    "            max_length = max(target_lengths)\n",
    "            batch[\"labels\"] = batch[\"labels\"][:, :max_length]\n",
    "\n",
    "        # also add in the speaker embeddings\n",
    "        batch[\"speaker_embeddings\"] = torch.tensor(speaker_features)\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = TTSDataCollatorWithPadding(processor=processor)\n",
    "\n",
    "features = [\n",
    "    dataset[\"train\"][0],\n",
    "    dataset[\"train\"][1],\n",
    "    dataset[\"train\"][20],\n",
    "]\n",
    "\n",
    "batch = data_collator(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77c76eab001f4079b7cb1b81e2bc72c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "                                        output_dir=\"models/sinhala-text-to-speech\",\n",
    "                                        per_device_train_batch_size=2,\n",
    "                                        gradient_accumulation_steps=2,\n",
    "                                        learning_rate=1e-5,\n",
    "                                        warmup_steps=500,\n",
    "                                        max_steps=150,\n",
    "                                        gradient_checkpointing=True,\n",
    "                                        fp16=False,\n",
    "                                        evaluation_strategy=\"steps\",\n",
    "                                        per_device_eval_batch_size=2,\n",
    "                                        save_steps=10,\n",
    "                                        eval_steps=10,\n",
    "                                        logging_steps=25,\n",
    "                                        report_to=[\"tensorboard\"],\n",
    "                                        load_best_model_at_end=True,\n",
    "                                        greater_is_better=False,\n",
    "                                        label_names=[\"labels\"],\n",
    "                                        push_to_hub=False,\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b12bb968d3054d61bcb0d3e0cb085f32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4663dd44036f46488b5d59142b2b9a47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/165 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8853644132614136, 'eval_runtime': 314.1971, 'eval_samples_per_second': 1.05, 'eval_steps_per_second': 0.525, 'epoch': 0.01}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e246a0d281bc4472b3469a7155dbd0fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/165 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8724579811096191, 'eval_runtime': 327.1562, 'eval_samples_per_second': 1.009, 'eval_steps_per_second': 0.504, 'epoch': 0.03}\n",
      "{'loss': 0.9561, 'learning_rate': 5.000000000000001e-07, 'epoch': 0.03}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cc8da51e1f449b6a11d7b15e9040c2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/165 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8518279194831848, 'eval_runtime': 344.6896, 'eval_samples_per_second': 0.957, 'eval_steps_per_second': 0.479, 'epoch': 0.04}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a91dc1eaae5a46f39933f82a82074658",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/165 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8223880529403687, 'eval_runtime': 318.666, 'eval_samples_per_second': 1.036, 'eval_steps_per_second': 0.518, 'epoch': 0.05}\n",
      "{'loss': 0.9022, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.07}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4c7f72d313a4629b4bddcb5b2b44945",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/165 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7877955436706543, 'eval_runtime': 352.8109, 'eval_samples_per_second': 0.935, 'eval_steps_per_second': 0.468, 'epoch': 0.07}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95e9e9d66acf46d0a1abab9802f78c05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/165 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7650370597839355, 'eval_runtime': 315.6893, 'eval_samples_per_second': 1.045, 'eval_steps_per_second': 0.523, 'epoch': 0.08}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "003961e66a654a76b414083e1f4e1061",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/165 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.753104567527771, 'eval_runtime': 306.7986, 'eval_samples_per_second': 1.076, 'eval_steps_per_second': 0.538, 'epoch': 0.09}\n",
      "{'loss': 0.8801, 'learning_rate': 1.5e-06, 'epoch': 0.1}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "454a46fe7ca448cea17fb7aa1114e92f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/165 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.734338104724884, 'eval_runtime': 314.7703, 'eval_samples_per_second': 1.048, 'eval_steps_per_second': 0.524, 'epoch': 0.11}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e22c00b938d496e9e31f0b39d2f2cf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/165 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7034872174263, 'eval_runtime': 317.8853, 'eval_samples_per_second': 1.038, 'eval_steps_per_second': 0.519, 'epoch': 0.12}\n",
      "{'loss': 0.8025, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.13}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "134f5497ad424b55a26ea5b884e6f31e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/165 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6901419162750244, 'eval_runtime': 320.6427, 'eval_samples_per_second': 1.029, 'eval_steps_per_second': 0.515, 'epoch': 0.13}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bcd8b3acdde4be9b1a325fab631c431",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/165 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6672263145446777, 'eval_runtime': 311.0776, 'eval_samples_per_second': 1.061, 'eval_steps_per_second': 0.53, 'epoch': 0.15}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b242288d7d742879680038992a05892",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/165 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6552231311798096, 'eval_runtime': 313.0018, 'eval_samples_per_second': 1.054, 'eval_steps_per_second': 0.527, 'epoch': 0.16}\n",
      "{'loss': 0.7311, 'learning_rate': 2.5e-06, 'epoch': 0.17}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1388370cac514c9d8a19b02b370943c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/165 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.650939404964447, 'eval_runtime': 320.9022, 'eval_samples_per_second': 1.028, 'eval_steps_per_second': 0.514, 'epoch': 0.18}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d5079bab13e4522b1e5805ef2c646fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/165 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6363164782524109, 'eval_runtime': 303.684, 'eval_samples_per_second': 1.087, 'eval_steps_per_second': 0.543, 'epoch': 0.19}\n",
      "{'loss': 0.7281, 'learning_rate': 3e-06, 'epoch': 0.2}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26105213abe245f080f79cc9dc5ab259",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/165 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6351116895675659, 'eval_runtime': 289.4955, 'eval_samples_per_second': 1.14, 'eval_steps_per_second': 0.57, 'epoch': 0.2}\n",
      "{'train_runtime': 7062.0068, 'train_samples_per_second': 0.085, 'train_steps_per_second': 0.021, 'train_loss': 0.8333395258585612, 'epoch': 0.2}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=150, training_loss=0.8333395258585612, metrics={'train_runtime': 7062.0068, 'train_samples_per_second': 0.085, 'train_steps_per_second': 0.021, 'train_loss': 0.8333395258585612, 'epoch': 0.2})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "                        args=training_args,\n",
    "                        model=model,\n",
    "                        train_dataset=dataset[\"train\"],\n",
    "                        eval_dataset=dataset[\"test\"],\n",
    "                        data_collator=data_collator,\n",
    "                        tokenizer=processor.tokenizer,\n",
    "                        )\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch113",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
