{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from evaluate import load\n",
    "import glob, os, uuid, json, torch\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import AutoProcessor, AutoModelForCausalLM, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TORCH VERSION: 1.13.1+cpu\n",
      "CUDA AVAILABLE: False\n"
     ]
    }
   ],
   "source": [
    "print(\"TORCH VERSION:\", torch.__version__)\n",
    "print(\"CUDA AVAILABLE:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_Dataset(data_dir = 'data/captioning-v2'):\n",
    "    all_files = glob.glob(f'{data_dir}/*/*.*')\n",
    "    image_files = [f for f in all_files if not f.endswith('.json')]\n",
    "    image_files = [file.replace('\\\\', '/') for file in image_files]\n",
    "    json_files = [f.split('.')[0] + '.json' for f in image_files]\n",
    "\n",
    "    for image_file, json_file in zip(image_files, json_files):\n",
    "        if os.path.exists(image_file) and os.path.exists(json_file):\n",
    "            if image_file.endswith('.jpg') or image_file.endswith('.png') or image_file.endswith('.jpeg') or image_file.endswith('.JPG') or image_file.endswith('.PNG') or image_file.endswith('.JPEG'):\n",
    "                file_dir, _ = os.path.split(image_file)\n",
    "                rand_name = str(uuid.uuid4())\n",
    "                new_image_file = os.path.join(file_dir, rand_name + '.jpg')\n",
    "                new_json_file = os.path.join(file_dir, rand_name + '.json')\n",
    "\n",
    "                os.rename(image_file, new_image_file)\n",
    "                os.rename(json_file, new_json_file)\n",
    "                \n",
    "# create_Dataset()\n",
    "\n",
    "def create_hf_datase(data_dir = 'data/captioning-v2'):\n",
    "    all_files = glob.glob(f'{data_dir}/*/*.*')\n",
    "    image_files = [f for f in all_files if not f.endswith('.json')]\n",
    "    image_files = [file.replace('\\\\', '/') for file in image_files]\n",
    "    json_files = [f.split('.')[0] + '.json' for f in image_files]\n",
    "\n",
    "\n",
    "    dataset = {}\n",
    "    dataset['image'] = []\n",
    "    dataset['text'] = []\n",
    "    for image_file, json_file in zip(image_files, json_files):\n",
    "        if os.path.exists(image_file) and os.path.exists(json_file):\n",
    "            with open(json_file, 'r') as f:\n",
    "                json_data = json.load(f)\n",
    "\n",
    "            caption = json_data['shapes'][0]['label']\n",
    "            image = Image.open(image_file)\n",
    "            image = image.convert('RGB')\n",
    "\n",
    "            dataset['image'].append(image)\n",
    "            dataset['text'].append(caption)\n",
    "\n",
    "    dataset = Dataset.from_dict(dataset)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['image', 'text'],\n",
       "    num_rows: 335\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = create_hf_datase()\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"microsoft/git-base\"\n",
    "processor = AutoProcessor.from_pretrained(checkpoint)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transforms(example_batch):\n",
    "    images = [x for x in example_batch[\"image\"]]\n",
    "    captions = [x for x in example_batch[\"text\"]]\n",
    "    inputs = processor(images=images, text=captions, padding=\"max_length\")\n",
    "    inputs.update({\"labels\": inputs[\"input_ids\"]})\n",
    "    return inputs\n",
    "\n",
    "dataset.set_transform(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "wer = load(\"wer\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predicted = logits.argmax(-1)\n",
    "    decoded_labels = processor.batch_decode(labels, skip_special_tokens=True)\n",
    "    decoded_predictions = processor.batch_decode(predicted, skip_special_tokens=True)\n",
    "    wer_score = wer.compute(predictions=decoded_predictions, references=decoded_labels)\n",
    "    return {\"wer_score\": wer_score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c73904521a24e8fbdc7de42969da36b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2505 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\thiro\\AudioSight\\model-train-backend\\image-captioning.ipynb Cell 8\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/thiro/AudioSight/model-train-backend/image-captioning.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m training_args \u001b[39m=\u001b[39m TrainingArguments(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/thiro/AudioSight/model-train-backend/image-captioning.ipynb#X10sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m                                 output_dir\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmodels/sinhala-book-captioning-repo\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/thiro/AudioSight/model-train-backend/image-captioning.ipynb#X10sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m                                 learning_rate\u001b[39m=\u001b[39m\u001b[39m5e-5\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/thiro/AudioSight/model-train-backend/image-captioning.ipynb#X10sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m                                 load_best_model_at_end\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/thiro/AudioSight/model-train-backend/image-captioning.ipynb#X10sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m                                 )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/thiro/AudioSight/model-train-backend/image-captioning.ipynb#X10sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/thiro/AudioSight/model-train-backend/image-captioning.ipynb#X10sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m                 model\u001b[39m=\u001b[39mmodel,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/thiro/AudioSight/model-train-backend/image-captioning.ipynb#X10sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m                 args\u001b[39m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/thiro/AudioSight/model-train-backend/image-captioning.ipynb#X10sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m                 compute_metrics\u001b[39m=\u001b[39mcompute_metrics,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/thiro/AudioSight/model-train-backend/image-captioning.ipynb#X10sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m                 )\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/thiro/AudioSight/model-train-backend/image-captioning.ipynb#X10sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[1;32mc:\\Users\\thiro\\anaconda3\\envs\\myenv\\lib\\site-packages\\transformers\\trainer.py:1544\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1541\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1542\u001b[0m     \u001b[39m# Disable progress bars when uploading models during checkpoints to avoid polluting stdout\u001b[39;00m\n\u001b[0;32m   1543\u001b[0m     hf_hub_utils\u001b[39m.\u001b[39mdisable_progress_bars()\n\u001b[1;32m-> 1544\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   1545\u001b[0m         args\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m   1546\u001b[0m         resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[0;32m   1547\u001b[0m         trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[0;32m   1548\u001b[0m         ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[0;32m   1549\u001b[0m     )\n\u001b[0;32m   1550\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m   1551\u001b[0m     hf_hub_utils\u001b[39m.\u001b[39menable_progress_bars()\n",
      "File \u001b[1;32mc:\\Users\\thiro\\anaconda3\\envs\\myenv\\lib\\site-packages\\transformers\\trainer.py:1893\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1888\u001b[0m         nn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mclip_grad_norm_(\n\u001b[0;32m   1889\u001b[0m             amp\u001b[39m.\u001b[39mmaster_params(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer),\n\u001b[0;32m   1890\u001b[0m             args\u001b[39m.\u001b[39mmax_grad_norm,\n\u001b[0;32m   1891\u001b[0m         )\n\u001b[0;32m   1892\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1893\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maccelerator\u001b[39m.\u001b[39;49mclip_grad_norm_(\n\u001b[0;32m   1894\u001b[0m             model\u001b[39m.\u001b[39;49mparameters(),\n\u001b[0;32m   1895\u001b[0m             args\u001b[39m.\u001b[39;49mmax_grad_norm,\n\u001b[0;32m   1896\u001b[0m         )\n\u001b[0;32m   1898\u001b[0m \u001b[39m# Optimizer step\u001b[39;00m\n\u001b[0;32m   1899\u001b[0m optimizer_was_run \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\thiro\\anaconda3\\envs\\myenv\\lib\\site-packages\\accelerate\\accelerator.py:2121\u001b[0m, in \u001b[0;36mAccelerator.clip_grad_norm_\u001b[1;34m(self, parameters, max_norm, norm_type)\u001b[0m\n\u001b[0;32m   2119\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   2120\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munscale_gradients()\n\u001b[1;32m-> 2121\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mnn\u001b[39m.\u001b[39;49mutils\u001b[39m.\u001b[39;49mclip_grad_norm_(parameters, max_norm, norm_type\u001b[39m=\u001b[39;49mnorm_type)\n",
      "File \u001b[1;32mc:\\Users\\thiro\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\utils\\clip_grad.py:43\u001b[0m, in \u001b[0;36mclip_grad_norm_\u001b[1;34m(parameters, max_norm, norm_type, error_if_nonfinite)\u001b[0m\n\u001b[0;32m     41\u001b[0m     total_norm \u001b[39m=\u001b[39m norms[\u001b[39m0\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(norms) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39melse\u001b[39;00m torch\u001b[39m.\u001b[39mmax(torch\u001b[39m.\u001b[39mstack(norms))\n\u001b[0;32m     42\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 43\u001b[0m     total_norm \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnorm(torch\u001b[39m.\u001b[39mstack([torch\u001b[39m.\u001b[39mnorm(g\u001b[39m.\u001b[39mdetach(), norm_type)\u001b[39m.\u001b[39mto(device) \u001b[39mfor\u001b[39;00m g \u001b[39min\u001b[39;00m grads]), norm_type)\n\u001b[0;32m     44\u001b[0m \u001b[39mif\u001b[39;00m error_if_nonfinite \u001b[39mand\u001b[39;00m torch\u001b[39m.\u001b[39mlogical_or(total_norm\u001b[39m.\u001b[39misnan(), total_norm\u001b[39m.\u001b[39misinf()):\n\u001b[0;32m     45\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m     46\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mThe total norm of order \u001b[39m\u001b[39m{\u001b[39;00mnorm_type\u001b[39m}\u001b[39;00m\u001b[39m for gradients from \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     47\u001b[0m         \u001b[39m'\u001b[39m\u001b[39m`parameters` is non-finite, so it cannot be clipped. To disable \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     48\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mthis error and scale the gradients by the non-finite norm anyway, \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     49\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mset `error_if_nonfinite=False`\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\thiro\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\utils\\clip_grad.py:43\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     41\u001b[0m     total_norm \u001b[39m=\u001b[39m norms[\u001b[39m0\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(norms) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39melse\u001b[39;00m torch\u001b[39m.\u001b[39mmax(torch\u001b[39m.\u001b[39mstack(norms))\n\u001b[0;32m     42\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 43\u001b[0m     total_norm \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnorm(torch\u001b[39m.\u001b[39mstack([torch\u001b[39m.\u001b[39;49mnorm(g\u001b[39m.\u001b[39;49mdetach(), norm_type)\u001b[39m.\u001b[39mto(device) \u001b[39mfor\u001b[39;00m g \u001b[39min\u001b[39;00m grads]), norm_type)\n\u001b[0;32m     44\u001b[0m \u001b[39mif\u001b[39;00m error_if_nonfinite \u001b[39mand\u001b[39;00m torch\u001b[39m.\u001b[39mlogical_or(total_norm\u001b[39m.\u001b[39misnan(), total_norm\u001b[39m.\u001b[39misinf()):\n\u001b[0;32m     45\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m     46\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mThe total norm of order \u001b[39m\u001b[39m{\u001b[39;00mnorm_type\u001b[39m}\u001b[39;00m\u001b[39m for gradients from \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     47\u001b[0m         \u001b[39m'\u001b[39m\u001b[39m`parameters` is non-finite, so it cannot be clipped. To disable \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     48\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mthis error and scale the gradients by the non-finite norm anyway, \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     49\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mset `error_if_nonfinite=False`\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\thiro\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\functional.py:1485\u001b[0m, in \u001b[0;36mnorm\u001b[1;34m(input, p, dim, keepdim, out, dtype)\u001b[0m\n\u001b[0;32m   1483\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(p, \u001b[39mstr\u001b[39m):\n\u001b[0;32m   1484\u001b[0m         _dim \u001b[39m=\u001b[39m [i \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(ndim)]  \u001b[39m# noqa: C416 TODO: rewrite as list(range(m))\u001b[39;00m\n\u001b[1;32m-> 1485\u001b[0m         \u001b[39mreturn\u001b[39;00m _VF\u001b[39m.\u001b[39;49mnorm(\u001b[39minput\u001b[39;49m, p, dim\u001b[39m=\u001b[39;49m_dim, keepdim\u001b[39m=\u001b[39;49mkeepdim)  \u001b[39m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m   1487\u001b[0m \u001b[39m# TODO: when https://github.com/pytorch/pytorch/issues/33782 is fixed\u001b[39;00m\n\u001b[0;32m   1488\u001b[0m \u001b[39m# remove the overloads where dim is an int and replace with BraodcastingList1\u001b[39;00m\n\u001b[0;32m   1489\u001b[0m \u001b[39m# and remove next four lines, replace _dim with dim\u001b[39;00m\n\u001b[0;32m   1490\u001b[0m \u001b[39mif\u001b[39;00m dim \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "                                output_dir=\"models/sinhala-book-captioning-repo\",\n",
    "                                learning_rate=5e-5,\n",
    "                                num_train_epochs=15,\n",
    "                                fp16=True if torch.cuda.is_available() else False,\n",
    "                                per_device_train_batch_size=1,\n",
    "                                per_device_eval_batch_size=1,\n",
    "                                gradient_accumulation_steps=2,\n",
    "                                save_total_limit=3,\n",
    "                                evaluation_strategy=\"steps\",\n",
    "                                eval_steps=50,\n",
    "                                save_strategy=\"steps\",\n",
    "                                save_steps=50,\n",
    "                                logging_steps=10,\n",
    "                                remove_unused_columns=False,\n",
    "                                push_to_hub=True,\n",
    "                                label_names=[\"labels\"],\n",
    "                                load_best_model_at_end=True,\n",
    "                                )\n",
    "trainer = Trainer(\n",
    "                model=model,\n",
    "                args=training_args,\n",
    "                train_dataset=dataset,\n",
    "                eval_dataset=dataset.train_test_split(test_size=0.2)[\"test\"],\n",
    "                compute_metrics=compute_metrics,\n",
    "                )\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b27144729824786912c479388325e8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/707M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8645dac4b79d4e4da1bcf94126604d60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/4.09k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b0b92b010ca44b3855949122882a98f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/thirosh0520/sinhala-book-captioning-repo/tree/main/'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub(\"sinhala-book-captioning\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thiro\\anaconda3\\envs\\myenv\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:479: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "c:\\Users\\thiro\\anaconda3\\envs\\myenv\\lib\\site-packages\\transformers\\models\\auto\\processing_auto.py:204: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL LOADED\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\"models/sinhala-book-captioning-repo\", use_auth_token=True)\n",
    "processor = AutoProcessor.from_pretrained(\"microsoft/git-base\", use_auth_token=True)\n",
    "model.to(device)\n",
    "\n",
    "print(\"MODEL LOADED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_inference_image(image):\n",
    "    if isinstance(image, str):\n",
    "        image = Image.open(image)\n",
    "        image = image.convert('RGB')\n",
    "    inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "    generated_ids = model.generate(pixel_values=inputs.pixel_values, max_length=5)\n",
    "    generated_caption = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    return generated_caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\thiro\\AudioSight\\model-train-backend\\image-captioning.ipynb Cell 13\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/thiro/AudioSight/model-train-backend/image-captioning.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m predictions \u001b[39m=\u001b[39m [preprocess_inference_image(image) \u001b[39mfor\u001b[39;00m image \u001b[39min\u001b[39;00m dataset[\u001b[39m\"\u001b[39;49m\u001b[39mimage\u001b[39;49m\u001b[39m\"\u001b[39;49m]]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/thiro/AudioSight/model-train-backend/image-captioning.ipynb#X15sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m truth \u001b[39m=\u001b[39m [caption \u001b[39mfor\u001b[39;00m caption \u001b[39min\u001b[39;00m dataset[\u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m]]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/thiro/AudioSight/model-train-backend/image-captioning.ipynb#X15sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m df_results \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame({\u001b[39m\"\u001b[39m\u001b[39mpredictions\u001b[39m\u001b[39m\"\u001b[39m: predictions, \u001b[39m\"\u001b[39m\u001b[39mtruth\u001b[39m\u001b[39m\"\u001b[39m: truth})\n",
      "File \u001b[1;32mc:\\Users\\thiro\\anaconda3\\envs\\myenv\\lib\\site-packages\\datasets\\arrow_dataset.py:2803\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2801\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, key):  \u001b[39m# noqa: F811\u001b[39;00m\n\u001b[0;32m   2802\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[39;00m\n\u001b[1;32m-> 2803\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem(key)\n",
      "File \u001b[1;32mc:\\Users\\thiro\\anaconda3\\envs\\myenv\\lib\\site-packages\\datasets\\arrow_dataset.py:2788\u001b[0m, in \u001b[0;36mDataset._getitem\u001b[1;34m(self, key, **kwargs)\u001b[0m\n\u001b[0;32m   2786\u001b[0m formatter \u001b[39m=\u001b[39m get_formatter(format_type, features\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_info\u001b[39m.\u001b[39mfeatures, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mformat_kwargs)\n\u001b[0;32m   2787\u001b[0m pa_subtable \u001b[39m=\u001b[39m query_table(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data, key, indices\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_indices \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_indices \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m-> 2788\u001b[0m formatted_output \u001b[39m=\u001b[39m format_table(\n\u001b[0;32m   2789\u001b[0m     pa_subtable, key, formatter\u001b[39m=\u001b[39;49mformatter, format_columns\u001b[39m=\u001b[39;49mformat_columns, output_all_columns\u001b[39m=\u001b[39;49moutput_all_columns\n\u001b[0;32m   2790\u001b[0m )\n\u001b[0;32m   2791\u001b[0m \u001b[39mreturn\u001b[39;00m formatted_output\n",
      "File \u001b[1;32mc:\\Users\\thiro\\anaconda3\\envs\\myenv\\lib\\site-packages\\datasets\\formatting\\formatting.py:629\u001b[0m, in \u001b[0;36mformat_table\u001b[1;34m(table, key, formatter, format_columns, output_all_columns)\u001b[0m\n\u001b[0;32m    627\u001b[0m python_formatter \u001b[39m=\u001b[39m PythonFormatter(features\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m)\n\u001b[0;32m    628\u001b[0m \u001b[39mif\u001b[39;00m format_columns \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 629\u001b[0m     \u001b[39mreturn\u001b[39;00m formatter(pa_table, query_type\u001b[39m=\u001b[39;49mquery_type)\n\u001b[0;32m    630\u001b[0m \u001b[39melif\u001b[39;00m query_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcolumn\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    631\u001b[0m     \u001b[39mif\u001b[39;00m key \u001b[39min\u001b[39;00m format_columns:\n",
      "File \u001b[1;32mc:\\Users\\thiro\\anaconda3\\envs\\myenv\\lib\\site-packages\\datasets\\formatting\\formatting.py:398\u001b[0m, in \u001b[0;36mFormatter.__call__\u001b[1;34m(self, pa_table, query_type)\u001b[0m\n\u001b[0;32m    396\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mformat_row(pa_table)\n\u001b[0;32m    397\u001b[0m \u001b[39melif\u001b[39;00m query_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcolumn\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 398\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mformat_column(pa_table)\n\u001b[0;32m    399\u001b[0m \u001b[39melif\u001b[39;00m query_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbatch\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    400\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mformat_batch(pa_table)\n",
      "File \u001b[1;32mc:\\Users\\thiro\\anaconda3\\envs\\myenv\\lib\\site-packages\\datasets\\formatting\\formatting.py:494\u001b[0m, in \u001b[0;36mCustomFormatter.format_column\u001b[1;34m(self, pa_table)\u001b[0m\n\u001b[0;32m    493\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mformat_column\u001b[39m(\u001b[39mself\u001b[39m, pa_table: pa\u001b[39m.\u001b[39mTable) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ColumnFormat:\n\u001b[1;32m--> 494\u001b[0m     formatted_batch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mformat_batch(pa_table)\n\u001b[0;32m    495\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(formatted_batch, \u001b[39m\"\u001b[39m\u001b[39mkeys\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    496\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(formatted_batch\u001b[39m.\u001b[39mkeys()) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\thiro\\anaconda3\\envs\\myenv\\lib\\site-packages\\datasets\\formatting\\formatting.py:515\u001b[0m, in \u001b[0;36mCustomFormatter.format_batch\u001b[1;34m(self, pa_table)\u001b[0m\n\u001b[0;32m    513\u001b[0m batch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpython_arrow_extractor()\u001b[39m.\u001b[39mextract_batch(pa_table)\n\u001b[0;32m    514\u001b[0m batch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpython_features_decoder\u001b[39m.\u001b[39mdecode_batch(batch)\n\u001b[1;32m--> 515\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(batch)\n",
      "\u001b[1;32mc:\\Users\\thiro\\AudioSight\\model-train-backend\\image-captioning.ipynb Cell 13\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/thiro/AudioSight/model-train-backend/image-captioning.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtransforms\u001b[39m(example_batch):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/thiro/AudioSight/model-train-backend/image-captioning.ipynb#X15sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     images \u001b[39m=\u001b[39m [x \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m example_batch[\u001b[39m\"\u001b[39m\u001b[39mimage\u001b[39m\u001b[39m\"\u001b[39m]]\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/thiro/AudioSight/model-train-backend/image-captioning.ipynb#X15sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     captions \u001b[39m=\u001b[39m [x \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m example_batch[\u001b[39m\"\u001b[39;49m\u001b[39mtext\u001b[39;49m\u001b[39m\"\u001b[39;49m]]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/thiro/AudioSight/model-train-backend/image-captioning.ipynb#X15sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     inputs \u001b[39m=\u001b[39m processor(images\u001b[39m=\u001b[39mimages, text\u001b[39m=\u001b[39mcaptions, padding\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmax_length\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/thiro/AudioSight/model-train-backend/image-captioning.ipynb#X15sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     inputs\u001b[39m.\u001b[39mupdate({\u001b[39m\"\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m\"\u001b[39m: inputs[\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m]})\n",
      "\u001b[1;31mKeyError\u001b[0m: 'text'"
     ]
    }
   ],
   "source": [
    "predictions = [preprocess_inference_image(image) for image in dataset[\"image\"]]\n",
    "truth = [caption for caption in dataset[\"text\"]]\n",
    "\n",
    "df_results = pd.DataFrame({\"predictions\": predictions, \"truth\": truth})\n",
    "df_results.to_csv(\"results.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/captioning-v2/herons and fish/2d99c093-3383-49d1-980e-bac67301e367.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\thiro\\AudioSight\\model-train-backend\\image-captioning.ipynb Cell 15\u001b[0m line \u001b[0;36m7\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/thiro/AudioSight/model-train-backend/image-captioning.ipynb#X20sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m model\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/thiro/AudioSight/model-train-backend/image-captioning.ipynb#X20sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m image_path \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mdata/captioning-v2/herons and fish/2d99c093-3383-49d1-980e-bac67301e367.jpg\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/thiro/AudioSight/model-train-backend/image-captioning.ipynb#X20sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m image \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39;49mopen(image_path)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/thiro/AudioSight/model-train-backend/image-captioning.ipynb#X20sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m image \u001b[39m=\u001b[39m image\u001b[39m.\u001b[39mconvert(\u001b[39m'\u001b[39m\u001b[39mRGB\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/thiro/AudioSight/model-train-backend/image-captioning.ipynb#X20sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m inputs \u001b[39m=\u001b[39m processor(images\u001b[39m=\u001b[39mimage, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[1;32mc:\\Users\\thiro\\anaconda3\\envs\\myenv\\lib\\site-packages\\PIL\\Image.py:3218\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   3215\u001b[0m     filename \u001b[39m=\u001b[39m fp\n\u001b[0;32m   3217\u001b[0m \u001b[39mif\u001b[39;00m filename:\n\u001b[1;32m-> 3218\u001b[0m     fp \u001b[39m=\u001b[39m builtins\u001b[39m.\u001b[39;49mopen(filename, \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m   3219\u001b[0m     exclusive_fp \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m   3221\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/captioning-v2/herons and fish/2d99c093-3383-49d1-980e-bac67301e367.jpg'"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\"models/sinhala-book-captioning-repo\", use_auth_token=True)\n",
    "processor = AutoProcessor.from_pretrained(\"microsoft/git-base\", use_auth_token=True)\n",
    "model.to(device)\n",
    "\n",
    "image_path = 'data/captioning-v2/herons and fish/2d99c093-3383-49d1-980e-bac67301e367.jpg'\n",
    "image = Image.open(image_path)\n",
    "image = image.convert('RGB')\n",
    "\n",
    "inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "generated_ids = model.generate(pixel_values=inputs.pixel_values, max_length=5)\n",
    "generated_caption = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "# visualize the image using PIL\n",
    "display(image)\n",
    "# print the caption generated by the model\n",
    "print(\"Generated caption:\", generated_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch113",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
