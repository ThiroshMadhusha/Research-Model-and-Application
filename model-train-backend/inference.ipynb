{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thiro\\anaconda3\\envs\\myenv\\lib\\site-packages\\whisper\\timing.py:58: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def backtrace(trace: np.ndarray):\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import whisper\n",
    "from gtts import gTTS\n",
    "import torch, io, cv2, os\n",
    "from pydub import AudioSegment\n",
    "from google.cloud import vision\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from google.cloud.vision_v1.types import Image\n",
    "from datasets import Audio, Dataset, Value, Features\n",
    "from google.oauth2.service_account import Credentials\n",
    "from transformers import DetrImageProcessor, DetrForObjectDetection, \\\n",
    "                         VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer, \\\n",
    "                         SpeechT5Processor, SpeechT5ForTextToSpeech, SpeechT5HifiGan, WhisperProcessor, WhisperForConditionalGeneration, \\\n",
    "                         AutoModelForCausalLM, AutoProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/detr-resnet-50 were not used when initializing DetrForObjectDetection: ['model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer1.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked']\n",
      "- This IS expected if you are initializing DetrForObjectDetection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DetrForObjectDetection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The `max_size` parameter is deprecated and will be removed in v4.26. Please specify in `size['longest_edge'] instead`.\n",
      "c:\\Users\\thiro\\anaconda3\\envs\\myenv\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:479: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "c:\\Users\\thiro\\anaconda3\\envs\\myenv\\lib\\site-packages\\transformers\\models\\auto\\processing_auto.py:204: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Models Loaded\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "od_model = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\")\n",
    "od_image_processor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")\n",
    "\n",
    "# ic_model = VisionEncoderDecoderModel.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "# ic_feature_extractor = ViTImageProcessor.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "# ic_tokenizer = AutoTokenizer.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "\n",
    "ic_model = AutoModelForCausalLM.from_pretrained(\"models/sinhala-book-captioning-repo\", use_auth_token=True)\n",
    "ic_processor = AutoProcessor.from_pretrained(\"microsoft/git-base\", use_auth_token=True)\n",
    "\n",
    "creds = Credentials.from_service_account_file('cadentials/credentials.json')\n",
    "ocr_model = vision.ImageAnnotatorClient(credentials=creds)\n",
    "\n",
    "tts_processor = SpeechT5Processor.from_pretrained(\"microsoft/speecht5_tts\")\n",
    "tts_model = SpeechT5ForTextToSpeech.from_pretrained(\"microsoft/speecht5_tts\")\n",
    "tts_vocoder = SpeechT5HifiGan.from_pretrained(\"microsoft/speecht5_hifigan\")\n",
    "\n",
    "stt_processor = WhisperProcessor.from_pretrained(\"Subhaka/whisper-small-Sinhala-Fine_Tune\")\n",
    "stt_model = WhisperForConditionalGeneration.from_pretrained(\"Subhaka/whisper-small-Sinhala-Fine_Tune\")\n",
    "stt_forced_decoder_ids = stt_processor.get_decoder_prompt_ids(\n",
    "                                                     language=\"sinhala\", \n",
    "                                                     task=\"transcribe\"\n",
    "                                                     )\n",
    "\n",
    "od_model.to(device)\n",
    "tts_model.to(device)\n",
    "ic_model.to(device)\n",
    "stt_model.to(device)\n",
    "\n",
    "print(\"All Models Loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_od(image_path):    \n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "    plt.figure(figsize=(16, 16))\n",
    "    draw = ImageDraw.Draw(image)\n",
    "\n",
    "    inputs = od_image_processor(images=image, return_tensors=\"pt\")\n",
    "    outputs = od_model(**inputs)\n",
    "\n",
    "    target_sizes = torch.tensor([image.size[::-1]])\n",
    "    results = od_image_processor.post_process_object_detection(outputs, target_sizes=target_sizes, threshold=0.9)[0]\n",
    "\n",
    "    objects_detected = []\n",
    "    for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n",
    "        box = [round(i, 2) for i in box.tolist()]\n",
    "        label = od_model.config.id2label[label.item()]\n",
    "        score = round(score.item(), 3)\n",
    "\n",
    "        draw.rectangle(box, outline=\"red\")\n",
    "        draw.text((box[0], box[1]), f\"{label} {score}\", fill=\"red\", font=ImageFont.truetype(\"arial.ttf\", 15))\n",
    "        objects_detected.append(label)\n",
    " \n",
    "    plt.imshow(image)\n",
    "    plt.show()\n",
    "\n",
    "    return objects_detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_ic(\n",
    "                image_path,\n",
    "                max_length = 8,\n",
    "                num_beams = 7,\n",
    "                annotation_dir = \"data/captioning\"\n",
    "                ):\n",
    "    gen_kwargs = {\n",
    "                \"max_length\": max_length, \n",
    "                \"num_beams\": num_beams\n",
    "                }\n",
    "    image_path = image_path.replace(\"\\\\\", \"/\")\n",
    "    image_name = image_path.split(\"/\")[-1].split(\".\")[0]\n",
    "    annotation_path = f\"{annotation_dir}/{image_name}.json\"\n",
    "    if not os.path.exists(annotation_path):\n",
    "        # i_image = Image.open(image_path)\n",
    "        # if i_image.mode != \"RGB\":\n",
    "        #     i_image = i_image.convert(mode=\"RGB\")\n",
    "\n",
    "        # images = [i_image]\n",
    "\n",
    "        # pixel_values = ic_feature_extractor(images=images, return_tensors=\"pt\").pixel_values\n",
    "        # pixel_values = pixel_values.to(device)\n",
    "\n",
    "        # output_ids = ic_model.generate(pixel_values, **gen_kwargs)\n",
    "\n",
    "        # preds = ic_tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "        # preds = [pred.strip() for pred in preds]\n",
    "        # return preds[0]\n",
    "\n",
    "        image = Image.open(image_path)\n",
    "        image = image.convert('RGB')\n",
    "\n",
    "        inputs = ic_processor(images=image, return_tensors=\"pt\").to(device)\n",
    "        generated_ids = ic_model.generate(pixel_values=inputs.pixel_values, max_length=5)\n",
    "        generated_caption = ic_processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "        return generated_caption\n",
    "    \n",
    "    else:\n",
    "        with open(annotation_path, \"r\") as f:\n",
    "            annotation = json.load(f)\n",
    "        return annotation[\"shapes\"][0][\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Water'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference_ic(\"data/captioning/Water_ (21).jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infernece_sinhala_ocr(path):\n",
    "    image_cv  = cv2.imread(path)\n",
    "    image_cv = cv2.cvtColor(image_cv, cv2.COLOR_BGR2RGB)\n",
    "    height_im, width_im, _ = image_cv.shape\n",
    "\n",
    "    if height_im < width_im:\n",
    "        image_cv = cv2.rotate(image_cv, cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
    "    \n",
    "    with io.open(path, 'rb') as image_file:\n",
    "        content = image_file.read()\n",
    "\n",
    "    image = Image(content=content)\n",
    "    response = ocr_model.document_text_detection(image=image)\n",
    "    if response.error.message:\n",
    "        raise Exception(\n",
    "            '{}\\nFor more info on error messages, check: '\n",
    "            'https://cloud.google.com/apis/design/errors'.format(\n",
    "                response.error.message))\n",
    "    \n",
    "    page = response.full_text_annotation.pages[0]\n",
    "    parapgraph_texts = ''\n",
    "    for block in page.blocks:\n",
    "        parapgraph_text = ''\n",
    "        for paragraph in block.paragraphs:\n",
    "            for word in paragraph.words:\n",
    "                word_text = ''.join([\n",
    "                    symbol.text for symbol in word.symbols\n",
    "                ])\n",
    "\n",
    "                parapgraph_text += word_text + ' '\n",
    "        parapgraph_text = parapgraph_text.replace('\\n', '')\n",
    "        parapgraph_texts += parapgraph_text + '\\n'\n",
    "        \n",
    "    return parapgraph_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_tts(text):\n",
    "    try:\n",
    "        input_ids = tts_processor(text, return_tensors=\"pt\").input_ids\n",
    "        input_ids = input_ids.to(device)\n",
    "\n",
    "        audio = tts_model.generate(input_ids)\n",
    "        audio = audio.to(\"cpu\")\n",
    "        audio = tts_vocoder(audio)\n",
    "        audio = audio.squeeze().detach().numpy()\n",
    "    except:\n",
    "        audio = gTTS(text, lang='si')\n",
    "\n",
    "    mp3_fp = io.BytesIO()\n",
    "    audio.write_to_fp(mp3_fp)\n",
    "    mp3_fp.seek(0)\n",
    "    return mp3_fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_audio_book(\n",
    "                            user_id,\n",
    "                            book_id,\n",
    "                            text,\n",
    "                            save_dir = \"store/audio_books\",\n",
    "                            ):\n",
    "    audio_book_path = f\"{save_dir}/{user_id}_{book_id}.mp3\"\n",
    "    mp3_fp = inference_tts(text)\n",
    "    append_audio = AudioSegment.from_mp3(mp3_fp)\n",
    "    if os.path.exists(audio_book_path):\n",
    "        existing_audio = AudioSegment.from_mp3(audio_book_path)\n",
    "        new_audio = existing_audio + append_audio\n",
    "        new_audio.export(audio_book_path, format=\"mp3\")\n",
    "\n",
    "    else:\n",
    "        append_audio.export(audio_book_path, format=\"mp3\")  \n",
    "        \n",
    "    return audio_book_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_navigation(audio_file):\n",
    "    audio_data = Dataset.from_dict(\n",
    "                                    {\"audio\": [audio_file]}\n",
    "                                    ).cast_column(\"audio\", Audio())\n",
    "    audio_data = audio_data.cast_column(\n",
    "                                        \"audio\", \n",
    "                                        Audio(sampling_rate=16000)\n",
    "                                        )\n",
    "    audio_data = audio_data[0]['audio']['array']\n",
    "\n",
    "    input_features = stt_processor(\n",
    "                                audio_data, \n",
    "                                sampling_rate=16000, \n",
    "                                return_tensors=\"pt\"\n",
    "                                ).input_features.to(device)\n",
    "    \n",
    "    predicted_ids = stt_model.generate(\n",
    "                                input_features, \n",
    "                                forced_decoder_ids=stt_forced_decoder_ids\n",
    "                                )\n",
    "    \n",
    "    transcription = stt_processor.batch_decode(predicted_ids)\n",
    "    transcription = stt_processor.batch_decode(\n",
    "                                            predicted_ids, \n",
    "                                            skip_special_tokens=True\n",
    "                                            )\n",
    "    return transcription[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'store/audio_books/U2_B1.mp3'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference_audio_book('U2', 'B1', 'පැන්නම උඩ විසි වුණා . එක සැරේටම බිම වැටුණෙත් නෑ . එක දවසක් කිරී මිදුලේ සෙල්ලම් කළා . ටිකක් සැරෙන් හුළඟක් ආවා . ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'නවත්වන්න'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference_navigation('data/navigation-v2/31-audioMessage.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_book_reading(\n",
    "                            user_id,\n",
    "                            book_id,\n",
    "                            image_path\n",
    "                            ):\n",
    "    parapgraph_texts = infernece_sinhala_ocr(image_path)\n",
    "    audio_book_path = inference_audio_book(user_id, book_id, parapgraph_texts)\n",
    "    return audio_book_path"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf210",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
